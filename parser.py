"""
è®¢é˜…é“¾æ¥è§£ææ¨¡å—
è´Ÿè´£ä¸‹è½½ã€è§£æå’Œæå–è®¢é˜…ä¿¡æ¯
"""

import base64
import re
import requests
import yaml
from urllib.parse import urlparse, parse_qs, unquote
from datetime import datetime

# ç¦ç”¨ SSL è­¦å‘Š
import urllib3
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)


class SubscriptionParser:
    """è®¢é˜…è§£æå™¨"""
    
    def __init__(self, proxy_port=7890, use_proxy=False):
        """
        åˆå§‹åŒ–è§£æå™¨
        
        Args:
            proxy_port: ä»£ç†ç«¯å£ï¼Œé»˜è®¤ 7890
            use_proxy: æ˜¯å¦ä½¿ç”¨ä»£ç†ï¼Œé»˜è®¤ False
        """
        self.proxy_port = proxy_port
        self.use_proxy = use_proxy
        
        if use_proxy:
            self.proxies = {
                'http': f'http://127.0.0.1:{proxy_port}',
                'https': f'http://127.0.0.1:{proxy_port}'
            }
        else:
            self.proxies = None
    
    def parse(self, url):
        """
        è§£æè®¢é˜…é“¾æ¥
        
        Args:
            url: è®¢é˜…é“¾æ¥
            
        Returns:
            dict: åŒ…å«è®¢é˜…ä¿¡æ¯çš„å­—å…¸
        """
        try:
            # ä¸‹è½½è®¢é˜…å†…å®¹
            response = self._download_subscription(url)
            
            # è§£ææµé‡ä¿¡æ¯ï¼ˆä»å“åº”å¤´ï¼‰
            traffic_info = self._parse_traffic_info(response.headers)
            
            # è§£æèŠ‚ç‚¹ä¿¡æ¯
            nodes = self._parse_nodes(response.text)
            
            # æå–æœºåœºåç§°ï¼ˆä¼˜å…ˆä»å“åº”å¤´ Content-Disposition æå–ï¼‰
            airport_name = self._extract_airport_name(nodes, url, response.headers)
            
            # ç»Ÿè®¡èŠ‚ç‚¹ä¿¡æ¯
            node_stats = self._analyze_nodes(nodes)
            
            # ç»„åˆç»“æœ (ä½å†…å­˜ä¼˜åŒ–ï¼šä¸å†ä¿å­˜æ‰€æœ‰çš„åŸå§‹èŠ‚ç‚¹é…ç½®ï¼Œåªéœ€ä¿å­˜ç»Ÿè®¡)
            result = {
                'name': airport_name,
                'node_count': len(nodes),
                'node_stats': node_stats,  # æ–°å¢ï¼šèŠ‚ç‚¹ç»Ÿè®¡
                **traffic_info
            }
            
            return result
            
        except requests.RequestException as e:
            raise Exception(f"ä¸‹è½½è®¢é˜…å¤±è´¥: {str(e)}")
        except Exception as e:
            raise Exception(f"è§£æè®¢é˜…å¤±è´¥: {str(e)}")
    
    def _download_subscription(self, url):
        """
        ä¸‹è½½è®¢é˜…å†…å®¹ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
        
        Args:
            url: è®¢é˜…é“¾æ¥
            
        Returns:
            Response: HTTP å“åº”å¯¹è±¡
        """
        from retry_utils import retry_on_failure
        
        # ä¼ªè£…æˆ Clash å®¢æˆ·ç«¯ï¼Œä»¥ä¾¿æœåŠ¡å™¨è¿”å›æµé‡ä¿¡æ¯
        headers = {
            'User-Agent': 'ClashForAndroid/2.5.12'
        }
        
        @retry_on_failure(max_retries=3, initial_delay=1.0, backoff_factor=2.0)
        def _fetch():
            response = requests.get(
                url,
                headers=headers,
                proxies=self.proxies,  # å¦‚æœ use_proxy=Falseï¼Œè¿™é‡Œä¼šæ˜¯ None
                timeout=30,
                verify=False  # è·³è¿‡ SSL éªŒè¯ï¼Œæ”¯æŒè‡ªç­¾è¯ä¹¦
            )
            response.raise_for_status()
            return response
        
        return _fetch()
    
    def _parse_traffic_info(self, headers):
        """
        ä»å“åº”å¤´è§£ææµé‡ä¿¡æ¯
        
        Args:
            headers: HTTP å“åº”å¤´
            
        Returns:
            dict: æµé‡ä¿¡æ¯å­—å…¸
        """
        traffic_info = {}
        
        # æŸ¥æ‰¾ subscription-userinfo å¤´
        userinfo = headers.get('subscription-userinfo', '')
        
        if userinfo:
            # è§£ææ ¼å¼: upload=xxx; download=xxx; total=xxx; expire=xxx
            parts = userinfo.split(';')
            for part in parts:
                part = part.strip()
                if '=' in part:
                    key, value = part.split('=', 1)
                    key = key.strip()
                    value = value.strip()
                    
                    if key in ['upload', 'download', 'total']:
                        traffic_info[key] = int(value)
                    elif key == 'expire':
                        try:
                            expire_timestamp = int(value)
                            expire_date = datetime.fromtimestamp(expire_timestamp)
                            traffic_info['expire_time'] = expire_date.strftime('%Y-%m-%d %H:%M:%S')
                        except:
                            pass
            
            # è®¡ç®—å·²ç”¨å’Œå‰©ä½™æµé‡
            if 'upload' in traffic_info and 'download' in traffic_info:
                traffic_info['used'] = traffic_info['upload'] + traffic_info['download']
            
            if 'total' in traffic_info and 'used' in traffic_info:
                traffic_info['remaining'] = traffic_info['total'] - traffic_info['used']
                
                # è®¡ç®—ä½¿ç”¨ç™¾åˆ†æ¯”
                if traffic_info['total'] > 0:
                    traffic_info['usage_percent'] = (traffic_info['used'] / traffic_info['total']) * 100
        
        return traffic_info
    
    def _parse_nodes(self, content):
        """
        è§£æèŠ‚ç‚¹ä¿¡æ¯ï¼ˆæ”¯æŒ Base64 å’Œ Clash YAML ä¸¤ç§æ ¼å¼ï¼‰
        
        Args:
            content: è®¢é˜…å†…å®¹
            
        Returns:
            list: èŠ‚ç‚¹åˆ—è¡¨
        """
        nodes = []
        MAX_NODES = 300  # å†…å­˜ä¼˜åŒ–ï¼šå¼ºåˆ¶æœ€å¤§è§£æèŠ‚ç‚¹æ•°ï¼Œé˜² OOM
        
        # æ£€æµ‹æ˜¯å¦ä¸º Clash YAML é…ç½®
        if content.strip().startswith('#') or 'proxies:' in content[:1000] or 'proxy-groups:' in content[:1000]:
            # å†…å­˜ä¼˜åŒ–ï¼šç›´æ¥æˆªæ–­è¶…è¿‡ 300KB çš„æ–‡ä»¶éƒ¨åˆ†ï¼ˆé€šå¸¸å¤Ÿå­˜å‡ åƒä¸ªèŠ‚ç‚¹äº†ï¼‰
            if len(content) > 300 * 1024:
                content = content[:300 * 1024]
                
            try:
                config = yaml.safe_load(content)
                if config and 'proxies' in config:
                    for proxy in config['proxies']:
                        if len(nodes) >= MAX_NODES:
                            break
                        if isinstance(proxy, dict):
                            node = {
                                'name': proxy.get('name', 'æœªçŸ¥èŠ‚ç‚¹'),
                                'protocol': proxy.get('type', 'unknown').lower(),
                                'server': proxy.get('server', ''),
                                'port': proxy.get('port', 0)
                            }
                            nodes.append(node)
                return nodes
            except Exception as e:
                # YAML è§£æå¤±è´¥ï¼Œå°è¯• Base64
                pass
        
        try:
            # å°è¯• Base64 è§£ç 
            decoded_content = base64.b64decode(content).decode('utf-8')
        except:
            # å¦‚æœè§£ç å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹
            decoded_content = content
        
        # æŒ‰è¡Œåˆ†å‰²
        lines = decoded_content.strip().split('\n')
        
        for line in lines:
            if len(nodes) >= MAX_NODES:
                break
                
            line = line.strip()
            if not line:
                continue
            
            # è¯†åˆ«ä¸åŒåè®®çš„èŠ‚ç‚¹
            node_info = self._parse_node_line(line)
            if node_info:
                nodes.append(node_info)
        
        return nodes
    
    def _parse_node_line(self, line):
        """
        è§£æå•ä¸ªèŠ‚ç‚¹è¡Œ
        
        Args:
            line: èŠ‚ç‚¹é…ç½®è¡Œ
            
        Returns:
            dict: èŠ‚ç‚¹ä¿¡æ¯ï¼Œå¦‚æœæ— æ³•è§£æåˆ™è¿”å› None
        """
        # æ”¯æŒçš„åè®®å‰ç¼€
        protocols = ['vmess://', 'vless://', 'ss://', 'ssr://', 'trojan://', 'hysteria://', 'hysteria2://']
        
        for protocol in protocols:
            if line.startswith(protocol):
                # æå–èŠ‚ç‚¹åç§°ï¼ˆé€šå¸¸åœ¨ # æˆ– remarks å‚æ•°ä¸­ï¼‰
                node_name = self._extract_node_name(line, protocol)
                
                return {
                    'protocol': protocol.replace('://', ''),
                    'name': node_name,
                    'raw': line
                }
        
        return None
    
    def _extract_node_name(self, line, protocol):
        """
        ä»èŠ‚ç‚¹é…ç½®ä¸­æå–èŠ‚ç‚¹åç§°
        
        Args:
            line: èŠ‚ç‚¹é…ç½®è¡Œ
            protocol: åè®®å‰ç¼€
            
        Returns:
            str: èŠ‚ç‚¹åç§°
        """
        # æ–¹æ³• 1: ä» # åé¢æå–
        if '#' in line:
            name = line.split('#', 1)[1]
            # URL è§£ç 
            try:
                from urllib.parse import unquote
                name = unquote(name)
            except:
                pass
            return name.strip()
        
        # æ–¹æ³• 2: ä» remarks å‚æ•°æå–ï¼ˆvmess ç­‰ï¼‰
        if protocol == 'vmess://':
            try:
                import json
                encoded = line.replace('vmess://', '')
                decoded = base64.b64decode(encoded).decode('utf-8')
                config = json.loads(decoded)
                if 'ps' in config:
                    return config['ps']
            except:
                pass
        
        return "æœªå‘½åèŠ‚ç‚¹"
    
    def _extract_airport_name(self, nodes, url, headers=None):
        """
        æå–æœºåœºåç§°
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            url: è®¢é˜…é“¾æ¥
            headers: å“åº”å¤´ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            str: æœºåœºåç§°
        """
        # æ–¹æ³• 1: ä»å“åº”å¤´ Content-Disposition æå–æ–‡ä»¶å
        if headers:
            cd = headers.get('Content-Disposition', '')
            if cd:
                try:
                    filename = None
                    if "filename*=" in cd:
                        # æå– filename*=UTF-8''xxx
                        part = cd.split("filename*=")[1].split(";")[0].strip()
                        if part.lower().startswith("utf-8''"):
                            encoded_name = part[7:]
                            filename = unquote(encoded_name)
                    elif "filename=" in cd:
                        # æå– filename="xxx"
                        filename = cd.split("filename=")[1].split(";")[0].strip('"')
                    
                    if filename:
                        # ç§»é™¤æ‰©å±•å
                        name = re.sub(r'\.(yaml|yml|txt|conf)$', '', filename, flags=re.IGNORECASE)
                        # ç§»é™¤å¸¸è§çš„æ‹¬å·åŒ…è£¹çš„å†…å®¹ï¼ˆå¦‚æœæ˜¯çº¯ä¿®é¥°æ€§çš„ï¼‰
                        # ä½†å¯¹äºã€69äº‘ã€‘è¿™ç§ï¼Œæˆ‘ä»¬å¸Œæœ›ä¿ç•™æˆ–æå–æ ¸å¿ƒéƒ¨åˆ†
                        # è¿™é‡Œç›´æ¥è¿”å›æ¸…ç†åçš„æ–‡ä»¶åï¼Œé€šå¸¸å°±æ˜¯æœºåœºå
                        return name.strip()
                except:
                    pass
        
        if not nodes:
            return "æœªçŸ¥æœºåœº"
        
        # æ–¹æ³• 2: ä»èŠ‚ç‚¹åç§°ä¸­æå–å…¬å…±å‰ç¼€
        node_names = [node['name'] for node in nodes if node.get('name')]
        
        if node_names:
            # æŸ¥æ‰¾å¸¸è§çš„æœºåœºåç§°æ¨¡å¼
            # ä¾‹å¦‚: "XXXæœºåœº - é¦™æ¸¯01", "XXXæœºåœº - æ—¥æœ¬02"
            common_patterns = []
            
            for name in node_names:
                # å°è¯•æå– "-" å‰çš„éƒ¨åˆ†
                if '-' in name:
                    prefix = name.split('-')[0].strip()
                    common_patterns.append(prefix)
                elif '|' in name:
                    prefix = name.split('|')[0].strip()
                    common_patterns.append(prefix)
            
            if common_patterns:
                # æ‰¾å‡ºæœ€å¸¸è§çš„å‰ç¼€
                from collections import Counter
                most_common = Counter(common_patterns).most_common(1)
                if most_common:
                    return most_common[0][0]
        
        # æ–¹æ³• 2: ä» URL ä¸­æå–åŸŸå
        try:
            parsed_url = urlparse(url)
            domain = parsed_url.netloc
            # ç§»é™¤å¸¸è§çš„å­åŸŸå
            domain = re.sub(r'^(www\.|api\.|sub\.)', '', domain)
            return domain
        except:
            pass
        
        return "æœªçŸ¥æœºåœº"
    
    def _analyze_nodes(self, nodes):
        """
        åˆ†æèŠ‚ç‚¹ç»Ÿè®¡ä¿¡æ¯(ä½¿ç”¨çœŸå®IPåœ°ç†ä½ç½®æŸ¥è¯¢)
        
        Args:
            nodes: èŠ‚ç‚¹åˆ—è¡¨
            
        Returns:
            dict: ç»Ÿè®¡ä¿¡æ¯(å›½å®¶/åœ°åŒºã€åè®®åˆ†å¸ƒã€è¯¦ç»†ä½ç½®)
        """
        from collections import Counter
        from node_extractor import NodeIPExtractor
        from geo_service import GeoLocationService
        
        # ç»Ÿè®¡åè®®
        protocols = [node.get('protocol', 'unknown') for node in nodes]
        protocol_stats = dict(Counter(protocols))
        
        # åˆå§‹åŒ–æœåŠ¡
        ip_extractor = NodeIPExtractor()
        geo_service = GeoLocationService()
        
        # ç»Ÿè®¡å›½å®¶/åœ°åŒº(ä½¿ç”¨çœŸå®IPæŸ¥è¯¢)
        countries = []
        locations_detail = []  # è¯¦ç»†ä½ç½®ä¿¡æ¯
        
        # å†…å­˜ä¼˜åŒ–ï¼šè®°å½•æ¯ä¸ªå›½å®¶å·²ç»ä¿å­˜çš„è¯¦ç»†èŠ‚ç‚¹æ•°ï¼Œä»…ä¿ç•™å‰3ä¸ªï¼Œä¸å†æŠŠæ‰€æœ‰300ä¸ªè¯¦æƒ…å¡è¿›å†…å­˜
        country_detail_count = Counter()
        
        geo_queries_count = 0
        MAX_GEO_QUERIES = 50  # é™åˆ¶æœ€å¤§æŸ¥è¯¢æ•°ï¼Œé˜²æ­¢è§¦å‘é™åˆ¶å’Œå¯¼è‡´åƒµæ­»
        
        for node in nodes:
            # æå–IP
            ip = ip_extractor.extract_ip(node)
            country = None
            detail_obj = None
            
            if ip and ip_extractor.is_valid_ip(ip) and geo_queries_count < MAX_GEO_QUERIES:
                geo_queries_count += 1
                # æŸ¥è¯¢åœ°ç†ä½ç½®
                location = geo_service.get_location(ip)
                if location:
                    country = location['country']
                    countries.append(country)
                    
                    if country_detail_count[country] < 3:
                        detail_obj = {
                            'name': node.get('name', 'æœªçŸ¥'),
                            'country': country,
                            'city': location['city'],
                            'isp': location['isp'],
                            'country_code': location['country_code'],
                            'flag': geo_service.get_country_flag(location['country_code'])
                        }
                    
            if not country:
                # å¦‚æœIPæŸ¥è¯¢å¤±è´¥,å›é€€åˆ°å…³é”®è¯åŒ¹é…
                node_name = node.get('name', '')
                country = self._match_country_by_keyword(node_name)
                countries.append(country)
                
                if country_detail_count[country] < 3:
                    detail_obj = {
                        'name': node.get('name', 'æœªçŸ¥'),
                        'country': country,
                        'city': 'æœªçŸ¥',
                        'isp': 'æœªçŸ¥',
                        'country_code': '',
                        'flag': 'ğŸŒ'
                    }
                    
            if detail_obj:
                locations_detail.append(detail_obj)
                country_detail_count[country] += 1
        
        country_stats = dict(Counter(countries))
        
        return {
            'protocols': protocol_stats,
            'countries': country_stats,
            'locations': locations_detail  # æ–°å¢:è¯¦ç»†ä½ç½®åˆ—è¡¨
        }
    
    def _match_country_by_keyword(self, node_name: str) -> str:
        """é€šè¿‡å…³é”®è¯åŒ¹é…å›½å®¶(å¤‡ç”¨æ–¹æ¡ˆ)"""
        country_keywords = {
            'é¦™æ¸¯': ['é¦™æ¸¯', 'HK', 'Hong Kong', 'Hongkong'],
            'å°æ¹¾': ['å°æ¹¾', 'TW', 'Taiwan'],
            'æ—¥æœ¬': ['æ—¥æœ¬', 'JP', 'Japan'],
            'ç¾å›½': ['ç¾å›½', 'US', 'USA', 'America'],
            'æ–°åŠ å¡': ['æ–°åŠ å¡', 'SG', 'Singapore'],
            'éŸ©å›½': ['éŸ©å›½', 'KR', 'Korea'],
        }
        
        for country, keywords in country_keywords.items():
            if any(kw in node_name for kw in keywords):
                return country
        
        return 'å…¶ä»–'

